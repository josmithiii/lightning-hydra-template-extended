% Created 2025-09-15 by jos from /l/asm/ and /l/mt/ADCxGather2025.tex

%% \begin{emptyslide}[toc={}]{}
%% %\vspace{\stretch{1}}
%% \vspace{-1.44em}
%% \myFigureRotateToBox{ADCxGather-2025-title-both}{-90}{\twidth}{\theight}{}
%% %\myFigureRotateToBox{ADCxGather-video-zoom-background}{-90}{\twidth}{\theight}{}
%% \vspace{\stretch{1}}
%% \end{emptyslide}

\section[toc={Lightning Hydra Template \emph{Ext}}]{{PyTorch Lightning Hydra Template \emph{Extended} (LHTE)}}

\begin{emptyslide}[toc={}]{}
%\vspace{-1.44em}
\vspace{-1.1pt}
\myFigureToBox{ADCxGather-2025-title-part1}{\twidth}{\theight}{}
%\myFigureRotateToBox{ADCxGather-2025-title-part1}{-90}{\twidth}{\theight}{}
%\myFigureRotateToBox{ADCxGather-2025-title-LHTE}{-90}{\twidth}{\theight}{}
\vspace{\stretch{1}}
\end{emptyslide}

\begin{slide}[\slideopts,toc={Lightning Hydra Template Ext}]{Lightning Hydra Template Extended (LHTE) --- New Architectures and Datasets}

% ADCxGather2025 abstract 2025-08-24

\vspace{-1em}
  
% Final onboarding: https://conference.audio.dev/wp-admin/post.php?post=25949&action=edit&var=registration

  \begin{itemize}

    \item Extends the \textbf{Pytorch Lightning + Hydra Template} with top \textbf{image-classification architectures} and \textbf{additional dataset formats}

    \mpitem LHTE began as a fork of the \emph{Lightning Hydra Template (LHT):} \url{https://github.com/ashleve/lightning-hydra-template}
    \begin{itemize}

      \mpitem \emph{PyTorch Lightning} streamlines deep-learning model development in
      numerous ways

      \mpitem \emph{Hydra} is a powerful \emph{configuration management framework} often used

      \mpitem An LHT \emph{example experiment} trains an MLP on MNIST (hand-written digits)

    \end{itemize}

    \mpitem LHTE adds \emph{more architectures:} CNNs,  ViT, ConvNeXt, EfficientNet

    \mpitem LHTE adds \emph{more datasets:} CIFAR-10, CIFAR-100, VIMH:

    \mpitem Variable Image Multi-Head (VIMH) format generalizes CIFAR-100:
    \begin{itemize}
      \mpitem Up to 65k x 65k images with up to 65k channels % NSTM point: (``stacked spectral representations'')
      \mpitem \emph{Multi-Head ready:} Up to 65k \emph{labeled parameters} (8-bit label, 8-bit value)
      \mpitem Example Use: \emph{synthesis parameters} ($\le$ 256 parameters with $\le$ 256 settings each)
      % pairs with \emph{multiple classification / regression heads}
      %% \begin{itemize}
      %%   \mpitem Up to 256 parameters (8 bits)
      %%   \mpitem Up to 256 values for each parameter (8 bits)
      %% \end{itemize}
      \mpitem \emph{Data loaders} for all supported datasets and model architectures
      \mpitem The \texttt{configs/experiment/} directory contains \emph{benchmark replications} % on MNIST, CIFAR, and VIMH datasets
      \mpitem Top-level \texttt{Makefile} contains $\approx 80$ \emph{make targets} for tests, training, experiments
    \end{itemize}
  \end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Original LHT}]{Original Lightning Hydra Template (LHT)}

  \emph{Open-Source PyTorch Lightning + Hydra Bootstrap}\\
  \url{https://github.com/ashleve/lightning-hydra-template}

  \begin{itemize}
    \mpitem PyTorch \emph{project skeleton} with \texttt{src/}, \texttt{configs/}, and \texttt{tests/}, ready for research

    \mpitem \emph{Chatbots love it} % (They've probably seen a lot of it)

    \mpitem \emph{Hydra workflow:} CLI overrides such as:\\
    \hspace{0.1em}\texttt{> python src/train.py trainer=gpu data.batch\_size=64}\\
    for every configuration value ($\approx 300$ --- thousands in the LHTE)
    % LHTE: (19,402 at last count: 2,383 in the main \texttt{configs/})

    \mpitem \emph{PyTorch Lightning baseline:} single-head MNIST MLP experiment --- \emph{that's it!} % , reproducible training loops, and logging hooks

    \mpitem \emph{Make targets} for testing, training, and admin % , Conda or pip installation paths, and template README for new repos

    \mpitem LHTE maintains \emph{compatibility} with existing LHT configs, datasets, and checkpoints

  \end{itemize}

\end{slide}

%% Nice intro slide from the new GPT5-Codex, but covered already:
%% \begin{slide}[\slideopts,toc={Why LHTE}]{Why Lightning Hydra Template \emph{Extended}?}
%%   \emph{Motivation:}
%%   \begin{itemize}
%%     \mpitem Beyond MLP, integrate SimpleCNN, ConvNeXt-V2, EfficientNet, ViT, SDN, and multihead MLPs ship side by side
%%     \mpitem Out-of-the-box datasets beyond MNIST: CIFAR-10/100, multihead synth suites, and the new Variable Image MultiHead (VIMH) format
%%     \mpitem Hydra-first overrides encourage rapid prototyping --- swap data, models, trainers, and losses without touching code
%%     \mpitem Developer ergonomics matter: 100+ make targets, scripted smoke tests, and diagram tooling keep the feedback loop tight
%%     \mpitem Everything remains compatible with upstream LHT so existing configs and checkpoints keep working
%%   \end{itemize}
%% \end{slide}
%%
%% Ditto - this was written from an intro to the code, so more aimed at that:
%% \begin{slide}[\slideopts,toc={LHTE Tour}]{LHTE in 5 Minutes}
%%   \emph{Hands-On Preview}
%%   \begin{itemize}
%%     \mpitem \texttt{sh setup.sh} to bootstrap a uv-managed virtual environment with PyTorch + Lightning
%%     \mpitem \texttt{make h} surfaces discoverable commands; \texttt{make tqa} runs MNIST smoke trains across every architecture
%%     \mpitem CIFAR baselines: \texttt{make cbqa} (quick sanity), \texttt{make cbs10/cbs100} (full benchmarks with logs)
%%     \mpitem Real multihead workloads: \texttt{python src/train.py experiment=vimh\_cnn} auto-configures heads from dataset metadata
%%     \mpitem Mix and match with CLI overrides such as \texttt{trainer.max\_epochs=1} or \texttt{model.loss=focal} to explore variants
%%   \end{itemize}
%% \end{slide}
%%
%% \begin{slide}[\slideopts,toc={Feature Map}]{Feature Map \& Documentation Trail}
%%   \emph{Where to Dive Deeper}
%%   \begin{itemize}
%%     \mpitem Architectures roundup: \texttt{docs/architectures.md} for parameter counts, diagrams, and recommended use cases
%%     \mpitem Dataset guide: \texttt{docs/vimh.md} + \texttt{docs/vimh\_cookbook.md} explain the metadata schema and sampling recipes
%%     \mpitem Benchmark ledger: \texttt{docs/benchmarks.md} documents CIFAR + VIMH comparisons with reproducible Hydra configs
%%     \mpitem Multihead strategy: \texttt{docs/multihead.md} covers head wiring, loss combinations, and evaluation metrics
%%     \mpitem Vision primer handout: \texttt{docs/vision\_primer.md} mirrors these slides for quick sharing without LaTeX
%%   \end{itemize}
%% \end{slide}
%%
%% Actually notes to ME:
%% \begin{slide}[\slideopts,toc={Talk Flow}]{Suggested Talk Flow}
%%   \emph{Use LHTE to Frame Your Story}
%%   \begin{itemize}
%%     \mpitem Start with the Lightning + Hydra reuse pitch, then highlight how LHTE supercharges it with modern backbones
%%     \mpitem Walk through the architecture menu and pair each backbone with a ready-to-run experiment
%%     \mpitem Demo quick wins: smoke tests, CIFAR comparisons, and VIMH runs with a few terminal commands
%%     \mpitem Land on multihead motivation and roadmap --- point to benchmark snapshots and upcoming extensions
%%     \mpitem Close by inviting the audience to clone the repo, open \texttt{docs/index.md}, and follow the handout for self-serve exploration
%%   \end{itemize}
%% \end{slide}

\input datasets.tex

\input architectures.tex

\begin{slide}[\slideopts,toc={MultiHead}]{Multi-Head Classification Support}

  \vspace{-1em}
  
  % \centerline{\includegraphics[width=0.5\textwidth]{viz/diagrams/multihead_architecture.eps}}
  \scalebox{0.7}{
    \input multihead.tex
  }

  \emph{Training Networks with Multiple Output Tasks}
  
  \begin{itemize}
    \mpitem \emph{Architecture:} Shared backbone â†’ separate heads per task
    
    \mpitem \emph{Task types:} Classification, ordinal regression, pure regression
    
    \mpitem \emph{Examples:} MNIST (digit+thickness), CIFAR-100 (fine+coarse),\\
    VIMH (synthesis-parameter + parameter-value)
    
    \mpitem Configurable via Hydra without code changes
  \end{itemize}
\end{slide}
%% \begin{slide}[\slideopts,toc={}]{Multi-Head Classification Support}
%%   \scalebox{1.0}{
%%     \input multihead.tex
%%   }
%% \end{slide}

\section[toc={Workflows}]{Benchmarks, Experiments, and Future Work}

\begin{slide}[\slideopts,toc={Benchmarks}]{Benchmark Replications \& Experiments}
  
  \emph{$\approx 30$ Pre-Configured Experiments}
  
  \begin{itemize}
    \mpitem \emph{Categories:} MNIST, CIFAR-10/100, VIMH, Multihead
    
    \mpitem \emph{Features:} Fixed seeds, fair comparisons, automated tracking
    
    \mpitem \emph{Example Make Commands} (abbreviated and full):
    \begin{itemize}
      \mpitem \texttt{make ca} \quad \quad (or \texttt{make compare-arch})
      \mpitem \texttt{make cbqa} \quad (or \texttt{make cifar-benchmark-quick-all})
      \mpitem \texttt{make cbsa} \quad (or \texttt{make cifar-benchmark-suite-all})
    \end{itemize}
    \mpitem Version-controlled experiment configs ensure reproducibility
  \end{itemize}
\end{slide}

\begin{wideslide}[\slideopts,toc={Experiments},method=direct]{LHTE Experiment Config Files}
{\footnotesize
\begin{verbatim}
cifar10_benchmark_cnn.yaml            cifar100_cnn.yaml             multihead_cnn_cifar10.yaml
cifar10_benchmark_convnext.yaml       cifar100_coarse_cnn.yaml      multihead_cnn_mnist.yaml
cifar10_benchmark_efficientnet.yaml   cifar100mh_cnn.yaml           vimh_cnn_16kdss_ordinal.yaml
cifar10_benchmark_vit.yaml            cifar100mh_convnext.yaml      vimh_cnn_16kdss_regression.yaml
cifar10_cnn_cpu.yaml                  cifar100mh_efficientnet.yaml  vimh_cnn_16kdss.yaml
cifar10_cnn.yaml                      cifar100mh_vit.yaml           vimh_cnn.yaml
cifar10_convnext_128k_optimized.yaml  cnn_mnist.yaml                vit_mnist_995.yaml
cifar10_convnext_64k_optimized.yaml   convnext_mnist.yaml           vit_mnist.yaml
cifar100_benchmark_cnn.yaml           convnext_v2_official...
cifar100_benchmark_convnext.yaml      example.yaml                  
\end{verbatim}
}
\textbf{LHT:} \; \texttt{mnist.yaml}
\end{wideslide}

%% TOO DOWN IN THE WEEDS
%% \begin{slide}[\slideopts,toc={Makefile}]{$\approx 80$ Makefile Targets}
%%   \emph{Streamlined Research Workflow}
%%   %% \begin{itemize}
%%   %%   \mpitem \emph{Abbreviations:} \texttt{t*} (train), \texttt{tq*} (quick), \texttt{cb*} (CIFAR), \texttt{e*} (experiments)
%%   %%   \mpitem \emph{Progressive testing:} \texttt{tq} â†’ \texttt{ca} â†’ \texttt{cbqa} â†’ \texttt{cbs}
%%   %%   \mpitem \emph{Productivity:} \texttt{make help}, logical naming, batch operations
%%   %% \end{itemize}
%% \end{slide}

\papersection{Future Work}{

  Next Steps:
  \begin{itemize}
  \item Merge PRs
  \item Chatbot Reviews (GPT, Claude, Gemini, \dots)
  \end{itemize}

  \vspace{2em}
  
  \centerline{\includegraphics[width=2cm]{eps/lhte_qr_code.eps}}
  \vspace{2pt}
  \centerline{{\scriptsize github.com/josmithiii/lightning-hydra-template-extended}}
}

\begin{slide}[\slideopts,toc={}]{Summary of Resources Online}
  \vspace{-1em}
  \begin{itemize}
    \item \emph{Neural Spectral Modeling Template (NSMT):}\\
      \href{https://github.com/josmithiii/neural-spectral-modeling-template.git}
           {\texttt{github.com/josmithiii/neural-spectral-modeling-template}}
    \item[]
    \item \emph{Lightning Hydra Template Extended (LHTE):}\\
      \href{https://github.com/josmithiii/lightning-hydra-template-extended.git}
           {\texttt{github.com/josmithiii/lightning-hydra-template-extended}}
    \item[]
    \item \emph{Lightning Hydra Template (LHT):}\\
      \href{https://github.com/ashleve/lightning-hydra-template}
           {\texttt{github.com/ashleve/lightning-hydra-template}}
    \item[]
    \item JOS Home Page (Videos, Overheads, including these):\\
      \href{https://ccrma.stanford.edu/~jos/Welcome.html}
           {\texttt{https://ccrma.stanford.edu/\~{}jos/}}
  \end{itemize}
  \hrule
  \vspace{2pt}
%  \centerline{
    {\scriptsize github.com/josmithiii/lightning-hydra-template-extended}\\
%  }
%  \centerline{
    \includegraphics[width=2cm]{eps/lhte_qr_code.eps}
%  }
\end{slide}
