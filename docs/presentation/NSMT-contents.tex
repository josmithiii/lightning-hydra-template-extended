\section[toc={Neural Spectral Modeling}]{Neural Spectral Modeling Template (NSMT)}
  
\begin{emptyslide}[toc={}]{}
\vspace{-1.44em}
\myFigureRotateToBox{ADCxGather-2025-title-part2}{-90}{\twidth}{\theight}{}
%\myFigureRotateToBox{ADCxGather-2025-title-NSMT}{-90}{\twidth}{\theight}{}
\vspace{\stretch{1}}
\end{emptyslide}

\begin{slide}[\slideopts,toc={Snapshot}]{Project Snapshot — Lightning Hydra Template Extended for Audio}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem Forked from LHTE (commit \texttt{3795aad}) and tuned for \textbf{spectral audio modeling}
    \mpitem Goal: \emph{small, accurate, fast} networks that leverage hearing-inspired priors
    \mpitem Hydra-driven config system; single command switches models, data, and trainer backends
    \mpitem Drops MNIST/CIFAR; embraces audio-ready tooling, logging, and diagnostics
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Layout}]{Repository Layout & Key Modules}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem Source in \texttt{src/}: \texttt{train.py}, \texttt{eval.py}, and packages for \texttt{data}, \texttt{models}, \texttt{utils}
    \mpitem Hydra configs under \texttt{configs/} (grouped by \texttt{data/}, \texttt{model/}, \texttt{trainer/}, \texttt{experiment/})
    \mpitem PyTorch Lightning modules live in \texttt{src/models/}; data modules in \texttt{src/data/}
    \mpitem Tests in \texttt{tests/} (fast by default, slow marked) with smoke checks for training loop
    \mpitem Assets flow: datasets \texttt{data/}, logs \texttt{logs/}, visualizations \texttt{viz/}, presentation/docs here
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Spectra}]{Spectral-First Audio Representations}
  Aren't we supposed to be doing everything \emph{end-to-end} now?

  Shouldn't the input be a digital audio \emph{bit stream} by now?

  \begin{itemize}
    \mpitem End-to-end works, but demands \emph{massive} data, compute, and time (e.g., giant Transformers)
    \mpitem NSMT embraces inductive priors: the ear is a hardware spectrum analyzer
    \mpitem Encode time-frequency trade-offs as stacked spectrogram ``channels''
    \mpitem Conditioning inputs bring in extra hints (pre-emphasis, modulation spectra, envelopes)
  \end{itemize}
  \vspace{-0.5em}
  \maybepause
    \centerline{\textit{Those who don't know signal processing are doomed to reinvent it}}
  \maybepause
    \centerline{\textit{Those who know signal processing are doomed to re-introduce it}}
\end{slide}

\begin{wideslide}[\slideopts,toc={Context}]{Anchoring in Human Audio Research}
  \twocolumn{
  \vspace{-2em}
  \myFigureRotateToWidth{McDermottKell2018}{-90}{\twidth}{}
  }{
  \vspace{-2em}
  \begin{itemize}
    \mpitem Kell et al. (Neuron 2018) used cochleagrams + CNNs to match human recognition
    \mpitem Shared convolutional front-end generalized across speech and music
    \mpitem Cochleagram input was essential to predict cortical responses
    \mpitem NSMT adopts similar spectral priors, adds multi-resolution stacks and conditioning heads
  \end{itemize}
}
\end{wideslide}

\begin{slide}[\slideopts,toc={VIMH}]{VIMH Dataset Format — Self-Describing, Multihead}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem \textbf{VIMH} (Variable Image MultiHead) stores H, W, C plus a variable-length label list per sample
    \mpitem JSON metadata \texttt{vimh\_dataset\_info.json} declares shapes, label ids, and ranges
    \mpitem Supports 1--255 quantized continuous parameters (0--255) and categorical tags
    \mpitem Single dataset auto-configures model input + output heads with minimal boilerplate
  \end{itemize}
  \vspace{0.25em}
  \begin{center}
    \begin{tikzpicture}[x=1mm,y=1mm, line join=round, line cap=round]

      \def\depthx{3}
      \def\depthy{1.2}

      \newcommand{\panel}[6][]{%
        \begin{scope}[shift={(#2,#3)}]
          \filldraw[fill=#6, draw=black] (0,0) rectangle (#4,#5);
          \filldraw[fill=black!20, draw=black]
            (#4,0) -- ++(\depthx,\depthy) -- ++(0,#5) -- ++(-\depthx,-\depthy) -- cycle;
          \ifx\\#1\\\else
            \node[font=\scriptsize, anchor=north] at (#4*0.5,-1.2) {#1};
          \fi
        \end{scope}%
      }

      \newcommand{\spectro}[6][]{%
        \begin{scope}[shift={(#2,#3)}]
          \filldraw[fill=#6, draw=black] (0,0) rectangle (#4,#5);
          \foreach \xx in {2,4,6,8,10,12,14,16,18}{\draw[gray!60] (\xx,0) -- (\xx,#5);} 
          \foreach \yy in {3,6,9,12,15,18,21,24}{\draw[gray!30] (0,\yy) -- (#4,\yy);} 
          \filldraw[fill=black!20, draw=black]
            (#4,0) -- ++(\depthx,\depthy) -- ++(0,#5) -- ++(-\depthx,-\depthy) -- cycle;
          \ifx\\#1\\\else
            \node[font=\scriptsize, anchor=north] at (#4*0.5,-1.2) {#1};
          \fi
        \end{scope}%
      }

      \def\xin{0}  \def\yin{0}
      \def\xbA{40} \def\ybA{-2}
      \def\xbB{72} \def\ybB{0}
      \def\xbC{98} \def\ybC{6}
      \def\xbD{118}\def\ybD{10}

      \spectro[32$\times$32$\times$4]{\xin}{\yin}{22}{26}{blue!12}
      \node[font=\scriptsize] at (\xin+10, \yin+30) {Stacked spectra};
      \filldraw[fill=gray!50, draw=black] (26,10) rectangle (38,16);
      \panel[Conv1]{\xbA}{\ybA}{18}{32}{orange!20}
      \filldraw[fill=gray!50, draw=black] (\xbA+18+\depthx,\ybA+12+\depthy) rectangle (\xbB-2,\ybB+18);
      \panel[Conv2]{\xbB}{\ybB}{16}{28}{green!20}
      \node[font=\scriptsize, anchor=north] at (\xbB+8,\ybB-2) {Pool};
      \filldraw[fill=gray!50, draw=black] (\xbB+16+\depthx,\ybB+12+\depthy) rectangle (\xbC-2,\ybC+18);
      \panel[FC]{\xbC}{\ybC}{12}{20}{violet!20}
      \filldraw[fill=gray!50, draw=black] (\xbC+12+\depthx,\ybC+9+\depthy) rectangle (\xbD-2,\ybD+14);
      \panel[Head 1]{\xbD}{\ybD}{6}{14}{gray!20}
      \panel[Head 2]{\xbD+8}{\ybD-1}{6}{12}{gray!20}
      \panel[Head 3]{\xbD+16}{\ybD-2}{6}{10}{gray!20}
      \node[font=\scriptsize, anchor=south] at (\xbD+9,\ybD+16) {Multihead outputs};

    \end{tikzpicture}
  \end{center}
\end{slide}

\begin{slide}[\slideopts,toc={Models}]{Model Portfolio & Auto-Configuration}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem CNN family: \texttt{cnn\_micro}, \texttt{cnn\_tiny}, \texttt{cnn\_64k}; variants for ordinal, regression, auxiliary heads
    \mpitem Lightweight ViTs: \texttt{vit\_micro}, \texttt{vit\_tiny} with patch embeddings tuned for spectrograms
    \mpitem Auto-wiring pipeline:
    \begin{itemize}
      \mpitem DataModule reads \texttt{vimh\_dataset\_info.json} and exposes shapes + label semantics
      \mpitem \texttt{VIMHLitModule} instantiates heads, losses, and metrics per label type
      \mpitem Hydra config selects trainer (CPU/GPU/MPS) and callbacks (checkpoints, early stop, diagrams)
    \end{itemize}
    \mpitem Outcome: one dataset file $\to$ consistent model wiring, reproducible experiments
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Losses}]{Losses, Metrics, and Diagnostics}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem Quantized parameters (0–255) demand distance-aware losses:
    \begin{itemize}
      \mpitem \texttt{OrdinalRegressionLoss}, \texttt{QuantizedRegressionLoss}, \texttt{WeightedCrossEntropyLoss}
    \end{itemize}
    \mpitem Metrics blend RMSE, correlation, perceptual error, and head-specific accuracy
    \mpitem Visual diagnostics: loss curves, per-parameter histograms, confusion matrices (in \texttt{viz/})
    \mpitem Logging via TensorBoard + CSV; configuration recorded in \texttt{logs/train/runs/}
  \end{itemize}
  \vspace{0.25em}
  \begin{center}
    \includegraphics[width=0.45\linewidth]{docs/png/vimh_loss_analysis.eps}\hspace{0.02\linewidth}
    \includegraphics[width=0.45\linewidth]{docs/png/vimh_perceptual_loss_analysis.eps}
  \end{center}
\end{slide}

\begin{slide}[\slideopts,toc={Workflow}]{Training & Experiment Workflow}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem \textbf{Datasets:} \texttt{make sds} (256 samples) \textbar \texttt{make sdl} (16k) \textbar \texttt{make ddr|dds|ddl}
    \mpitem \textbf{Training:} \texttt{make tq} (smoke) \textbar \texttt{make tr} (default) \textbar \texttt{python src/train.py experiment=emb trainer=gpu}
    \mpitem \textbf{Overrides:} specify \texttt{model=cnn\_64k\_ordinal}, \texttt{data=vimh}, \texttt{trainer.max\_epochs=50}
    \mpitem \textbf{Artifacts:} checkpoints + configs in run dirs; diagrams via \texttt{make td}, TensorBoard via \texttt{make tensorboard}
    \mpitem \textbf{Testing loop:} \texttt{pytest -k "not slow"} before commits, \texttt{make test-all} for release candidates
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Evaluation}]{Evaluation & Audio Reconstruction Tools}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem \texttt{python src/audio\_reconstruction\_eval.py} (or \texttt{make ae}) re-synthesizes audio from predictions
    \mpitem Auto-discovers latest checkpoint, launches interactive widget for A/B playback
    \mpitem Metrics: SNR, correlation, RMSE, per-parameter errors; export tables for papers
    \mpitem Supports batch comparisons and logging snapshots back to \texttt{logs/}
  \end{itemize}
  \vspace{0.25em}
  \begin{center}
    \setlength{\fboxsep}{6pt}\fbox{\parbox{0.88\linewidth}{
        spectrogram $\to$ model $\to$ predicted params $\to$ \textit{synth} $\to$ audio $\leftrightarrow$ compare with ground truth
    }}
  \end{center}
\end{slide}

\begin{slide}[\slideopts,toc={Experiments}]{Moog VCF Case Studies — Basic, Envelope, Resonance}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem \textbf{Datasets:} \texttt{sdmb} (4 params), \texttt{sdme} (10 params), \texttt{sdmr} (8 params)
    \mpitem \textbf{Experiments:} \texttt{emb}, \texttt{eme}, \texttt{emr} with CNN + ViT variants (\texttt{emvit*})
    \mpitem Probe generalization across parameter spaces and temporal envelopes
    \mpitem Findings: ordinal losses stabilize extrapolation; ViTs excel on wider envelopes, CNNs win on jittered resonance
  \end{itemize}
  \vspace{0.25em}
  \begin{center}
    \setlength{\fboxsep}{4pt}\fbox{\parbox{0.9\linewidth}{
        Color map: parameter sweeps $\to$ spectral patterns $\to$ multihead predictions for downstream control
    }}
  \end{center}
\end{slide}

\begin{slide}[\slideopts,toc={Practices}]{Engineering Practices, Reproducibility, Next Steps}
  \vspace{-0.75em}
  \begin{itemize}
    \mpitem Hydra configs capture overrides; log each run's command + git SHA in \texttt{logs/train/runs/}
    \mpitem Preflight scripts check label diversity and dataset integrity before training
    \mpitem Style guardrails: \texttt{make format} (black, isort, flake8, bandit); tests via \texttt{pytest}
    \mpitem Contribution checklist: document configs, attach sample logs/audio, link issues, verify smoke tests
    \mpitem Roadmap: expand VIMH converters, add lightweight diffusion head, refine interactive eval UX
  \end{itemize}
  \vspace{0.25em}
  \begin{center}
    \includegraphics[width=0.65\linewidth]{docs/png/confusion_matrices.eps}
  \end{center}
\end{slide}
