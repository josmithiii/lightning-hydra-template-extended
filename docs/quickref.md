# Quick Reference

Expert power-user cheat sheet. No fluff.

## Make Targets (Essential)
```bash
# SETUP
make h            # Show all targets
make lc           # List configs

# QUICK TESTS (1 epoch, limited batches)
make tq           # SimpleDenseNet MNIST
make tqc          # CNN MNIST
make tqv          # ViT MNIST
make tqcn         # ConvNeXt MNIST
make tqa          # All architectures

# BENCHMARKS
make cb10c        # CIFAR-10 CNN (85-92%)
make cb10cn       # CIFAR-10 ConvNeXt (90-95%)
make cb10v        # CIFAR-10 ViT (88-93%)
make cb100c       # CIFAR-100 CNN (55-70%)
make cbqa         # Quick all CIFAR validation
make cbs10        # Complete CIFAR-10 suite
make cbs100       # Complete CIFAR-100 suite

# EXPERIMENTS
make ecm          # CNN MNIST experiment (~99.1%)
make emhcm        # Multihead CNN MNIST
make evit         # ViT experiment
make ev995        # ViT SOTA 99.5% MNIST
make evimh        # VIMH CNN experiment

# COMPARISON & ANALYSIS
make ca           # Compare architectures (3 epochs each)
make td           # Generate model diagrams
make tds          # Simple text diagrams
make tb           # Launch TensorBoard

# UTILITIES
make t            # Fast tests (exclude slow)
make f            # Format code (pre-commit)
make c            # Clean autogenerated files
```

## Architectures
| Name | Type | Params | Best For | Config |
|------|------|--------|----------|--------|
| SimpleDenseNet | MLP | 8K-68K | Quick prototyping | `mnist_sdn_*` |
| SimpleCNN | CNN | 8K-3.3M | Image classification | `mnist_cnn_*` |
| ConvNeXt-V2 | Modern CNN | 18K-725K | Efficient performance | `mnist_convnext_*` |
| ViT | Transformer | 38K-821K | Large datasets | `mnist_vit_*` |
| EfficientNet | Mobile CNN | 22K-7M | Edge deployment | `mnist_efficientnet_*` |

## Command Patterns
```bash
# Architecture switching
python src/train.py model=mnist_cnn_421k
python src/train.py model=cifar10_convnext_210k

# Hardware selection (Mac users: always use mps)
python src/train.py trainer=mps
python src/train.py trainer=gpu
python src/train.py trainer=cpu

# Quick overrides
python src/train.py trainer.max_epochs=20 data.batch_size=64
python src/train.py +trainer.limit_train_batches=10

# Experiments (50+ available)
python src/train.py experiment=cnn_mnist
python src/train.py experiment=cifar10_benchmark_cnn
python src/train.py experiment=vit_mnist_995

# VIMH multihead
python src/train.py experiment=vimh_cnn_16kdss
python src/train.py experiment=vimh_cnn_16kdss_ordinal
```

## Hydra Overrides Cheat Sheet
```bash
# Override nested values
python src/train.py model.optimizer.lr=1e-3 trainer.max_epochs=5

# Add new keys with '+'
python src/train.py +trainer.limit_train_batches=10 +trainer.limit_val_batches=5

# Switch config groups
python src/train.py model=mnist_vit_38k data=mnist trainer=mps

# Combine experiment with overrides
python src/train.py experiment=cifar10_benchmark_cnn trainer.max_epochs=50 data.batch_size=128

# Change logger (see configs/logger/)
python src/train.py logger=tensorboard
```

## Config Group Map
- Data (`configs/data/`):
  - MNIST → [configs/data/mnist.yaml](../configs/data/mnist.yaml)
  - CIFAR-10 → [configs/data/cifar10.yaml](../configs/data/cifar10.yaml)
  - CIFAR-100 → [configs/data/cifar100.yaml](../configs/data/cifar100.yaml)
  - Multihead (MNIST/CIFAR-10) → [configs/data/multihead_mnist.yaml](../configs/data/multihead_mnist.yaml), [configs/data/multihead_cifar10.yaml](../configs/data/multihead_cifar10.yaml)
  - CIFAR-100-MH → [configs/data/cifar100mh.yaml](../configs/data/cifar100mh.yaml)
  - VIMH / VIMH (16K) → [configs/data/vimh.yaml](../configs/data/vimh.yaml), [configs/data/vimh_16kdss.yaml](../configs/data/vimh_16kdss.yaml)
- Model (`configs/model/`):
  - MNIST → [mnist_cnn_68k](../configs/model/mnist_cnn_68k.yaml), [mnist_vit_38k](../configs/model/mnist_vit_38k.yaml), [mnist_efficientnet_22k](../configs/model/mnist_efficientnet_22k.yaml), [mnist_mh_cnn_422k](../configs/model/mnist_mh_cnn_422k.yaml)
  - CIFAR-10 → [cifar10_cnn_64k](../configs/model/cifar10_cnn_64k.yaml), [cifar10_convnext_210k](../configs/model/cifar10_convnext_210k.yaml), [cifar10_vit_210k](../configs/model/cifar10_vit_210k.yaml), [cifar10_efficientnet_210k](../configs/model/cifar10_efficientnet_210k.yaml), [cifar10_mh_cnn_64k](../configs/model/cifar10_mh_cnn_64k.yaml)
  - VIMH → [vimh_cnn_64k](../configs/model/vimh_cnn_64k.yaml), [vimh_cnn_64k_ordinal](../configs/model/vimh_cnn_64k_ordinal.yaml), [vimh_cnn_64k_regression](../configs/model/vimh_cnn_64k_regression.yaml)
- Trainer (`configs/trainer/`):
  - CPU → [configs/trainer/cpu.yaml](../configs/trainer/cpu.yaml)
  - GPU (CUDA) → [configs/trainer/gpu.yaml](../configs/trainer/gpu.yaml)
  - MPS (macOS) → [configs/trainer/mps.yaml](../configs/trainer/mps.yaml)

Examples:
```bash
# Switch data/model/trainer groups
python src/train.py data=cifar10 model=cifar10_convnext_210k trainer=gpu

# MNIST ViT on Mac with quick limits
python src/train.py data=mnist model=mnist_vit_38k trainer=mps +trainer.limit_train_batches=10

# Multihead examples
python src/train.py data=multihead_mnist model=mnist_mh_cnn_422k trainer=mps
python src/train.py data=vimh model=vimh_cnn_64k trainer=cpu
```

## Expected Performance
| Dataset | Architecture | Expected | Training Time |
|---------|-------------|----------|---------------|
| MNIST | CNN | ~99.1% | 5 min |
| MNIST | ViT (SOTA) | 99.5% | 45 min |
| CIFAR-10 | CNN | 85-92% | 2h |
| CIFAR-10 | ConvNeXt | 90-95% | 1.5h |
| CIFAR-100 | CNN | 55-70% | 3h |
| CIFAR-100 | ConvNeXt | 70-80% | 2h |

## File Locations
```
configs/
├── experiment/     # 50+ complete experiments
├── model/         # Architecture definitions
├── data/          # MNIST, CIFAR-10/100, VIMH
└── trainer/       # cpu, gpu, mps, ddp

src/
├── train.py       # Main training script
├── models/        # Lightning modules
└── data/          # Data modules

logs/train/runs/   # Training outputs
data/              # Datasets (auto-download)
```

## Common Workflows
```bash
# Daily development
make f && make t && make tq

# Architecture exploration
make tqa && make ca

# Benchmark validation
make cbqa

# Research experiment
python src/train.py experiment=X tags='["research","comparison"]'

# Custom multihead
python src/train.py model=mnist_cnn data=vimh_16kdss
```

## Troubleshooting
- **"No module named 'rootutils'"** → `source .venv/bin/activate`
- **Mac users** → Always use `trainer=mps`
- **VIMH training** → Set `num_workers: 0` (MPS limitation)
- **Memory issues** → Reduce batch size: `data.batch_size=32`
- **Slow training** → Use quick tests first: `make tq*`
- **First-run downloads** → MNIST/CIFAR auto-download on first use
- **Checkpoints** → Remote URLs are blocked; use local files only
