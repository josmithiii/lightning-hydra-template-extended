# Quick Reference

Expert power-user cheat sheet. No fluff.

## Make Targets (Essential)
```bash
# SETUP
make h            # Show all targets
make lc           # List configs

# QUICK TESTS (1 epoch, limited batches)
make tq           # SimpleDenseNet MNIST
make tqc          # CNN MNIST
make tqv          # ViT MNIST
make tqcn         # ConvNeXt MNIST
make tqa          # All architectures

# BENCHMARKS
make cb10c        # CIFAR-10 CNN (85-92%)
make cb10cn       # CIFAR-10 ConvNeXt (90-95%)
make cb10v        # CIFAR-10 ViT (88-93%)
make cb100c       # CIFAR-100 CNN (55-70%)
make cbqa         # Quick all CIFAR validation
make cbs10        # Complete CIFAR-10 suite
make cbs100       # Complete CIFAR-100 suite

# EXPERIMENTS
make ecm          # CNN MNIST experiment (~99.1%)
make emhcm        # Multihead CNN MNIST
make evit         # ViT experiment
make ev995        # ViT SOTA 99.5% MNIST
make evimh        # VIMH CNN experiment

# COMPARISON & ANALYSIS
make ca           # Compare architectures (3 epochs each)
make td           # Generate model diagrams
make tds          # Simple text diagrams
make tb           # Launch TensorBoard

# UTILITIES
make t            # Fast tests (exclude slow)
make f            # Format code (pre-commit)
make c            # Clean autogenerated files
```

## Architectures
| Name | Type | Params | Best For | Config |
|------|------|--------|----------|--------|
| SimpleDenseNet | MLP | 8K-68K | Quick prototyping | `mnist_sdn_*` |
| SimpleCNN | CNN | 8K-3.3M | Image classification | `mnist_cnn_*` |
| ConvNeXt-V2 | Modern CNN | 18K-725K | Efficient performance | `mnist_convnext_*` |
| ViT | Transformer | 38K-821K | Large datasets | `mnist_vit_*` |
| EfficientNet | Mobile CNN | 22K-7M | Edge deployment | `mnist_efficientnet_*` |

## Command Patterns
```bash
# Architecture switching
python src/train.py model=mnist_cnn_421k
python src/train.py model=cifar10_convnext_210k

# Hardware selection (Mac users: always use mps)
python src/train.py trainer=mps
python src/train.py trainer=gpu
python src/train.py trainer=cpu

# Quick overrides
python src/train.py trainer.max_epochs=20 data.batch_size=64
python src/train.py +trainer.limit_train_batches=10

# Experiments (50+ available)
python src/train.py experiment=cnn_mnist
python src/train.py experiment=cifar10_benchmark_cnn
python src/train.py experiment=vit_mnist_995

# VIMH multihead
python src/train.py experiment=vimh_cnn_16kdss
python src/train.py experiment=vimh_cnn_16kdss_ordinal
```

## Expected Performance
| Dataset | Architecture | Expected | Training Time |
|---------|-------------|----------|---------------|
| MNIST | CNN | ~99.1% | 5 min |
| MNIST | ViT (SOTA) | 99.5% | 45 min |
| CIFAR-10 | CNN | 85-92% | 2h |
| CIFAR-10 | ConvNeXt | 90-95% | 1.5h |
| CIFAR-100 | CNN | 55-70% | 3h |
| CIFAR-100 | ConvNeXt | 70-80% | 2h |

## File Locations
```
configs/
├── experiment/     # 50+ complete experiments
├── model/         # Architecture definitions
├── data/          # MNIST, CIFAR-10/100, VIMH
└── trainer/       # cpu, gpu, mps, ddp

src/
├── train.py       # Main training script
├── models/        # Lightning modules
└── data/          # Data modules

logs/train/runs/   # Training outputs
data/              # Datasets (auto-download)
```

## Common Workflows
```bash
# Daily development
make f && make t && make tq

# Architecture exploration
make tqa && make ca

# Benchmark validation
make cbqa

# Research experiment
python src/train.py experiment=X tags='["research","comparison"]'

# Custom multihead
python src/train.py model=mnist_cnn data=vimh_16kdss
```

## Troubleshooting
- **"No module named 'rootutils'"** → `source .venv/bin/activate`
- **Mac users** → Always use `trainer=mps`
- **VIMH training** → Set `num_workers: 0` (MPS limitation)
- **Memory issues** → Reduce batch size: `data.batch_size=32`
- **Slow training** → Use quick tests first: `make tq*`
